# -*- coding: utf-8 -*-
"""Model NLP dengan TensorFlow.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rHAIW-NjBMc6go775MgSb9wWI01gj4ND
"""

#   Import library 
import tensorflow as tf
import numpy as np
import pandas as pd
from tensorflow import keras

#   Import dataset
df = pd.read_csv('English_Dataset.csv')

#   Cek shape pada dataset
df.shape

#   Cek nama kolom
df.columns

df.info()

#   Delete kolom yang tidak relevan untuk digunakan
df = df.drop(columns=['ArticleId'])

#   Cek 5 baris pertama pada data
df.head()

import re

text = "hey amazon - my package never arrived https://www.amazon.com/gp/css/order-history?ref_=nav_orders_first please fix asap! @amazonhelp"
text = re.sub(r"(@\[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)|^rt|http.+?", "", text)
print(text)

import pandas as pd
import numpy as np
from nltk.corpus import stopwords
import re
import nltk
nltk.download('stopwords')
nltk.download('wordnet')
from nltk.stem import PorterStemmer
from nltk.stem import LancasterStemmer

def clean_lower(lwr):
    lwr = lwr.lower() # lowercase text
    return lwr
# Buat kolom tambahan untuk data description yang telah dicasefolding  
df['lwr'] = df['Text'].apply(clean_lower)
casefolding=pd.DataFrame(df['lwr'])
casefolding

#   Remove Puncutuation
clean_spcl = re.compile('[/(){}\[\]\|@,;]')
clean_symbol = re.compile('[^0-9a-z]')
def clean_punct(text):
    text = clean_spcl.sub('', text)
    text = clean_symbol.sub(' ', text)
    return text
#   Buat kolom tambahan untuk data description yang telah diremovepunctuation   
df['clean_punct'] = df['lwr'].apply(clean_punct)
df['clean_punct']

import pandas as pd
import numpy as np
from nltk.corpus import stopwords
import re
import nltk
nltk.download('stopwords')
nltk.download('wordnet')
from nltk.stem import PorterStemmer
from nltk.stem import LancasterStemmer

#   Clean stopwords
stopword = set(stopwords.words('english'))
def clean_stopwords(text):
    text = ' '.join(word for word in text.split() if word not in stopword) # hapus stopword dari kolom deskripsi
    return text
#   Buat kolom tambahan untuk data description yang telah distopwordsremoval   
df['clean_punct'] = df['clean_punct'].apply(clean_stopwords)

add = pd.DataFrame(df['clean_punct'])
df['add_swr']= add.replace(to_replace =['whether','yes','also','thanks','take','whatever',
                                        'making','makes','taking','takes','ok','oh','etc',
                                        "yep"],
                           value ="", regex= True) 
df['add_swr']

df.head()

#   Melakukan one-hot encoding
category = pd.get_dummies(df.Category)
df_baru = pd.concat([df, category], axis=1)
df_baru = df_baru.drop(columns='Category')
df_baru

berita = df_baru['add_swr'].values
label = df_baru[['business', 'entertainment', 'politics', 'sport', 'tech']].values

#   Validation set sebesar 20% dari total dataset
from sklearn.model_selection import train_test_split
berita_latih, berita_test, label_latih, label_test = train_test_split(berita, label, test_size=0.2)

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
 
tokenizer = Tokenizer(num_words=5000, oov_token='<oov>')
tokenizer.fit_on_texts(berita_latih) 
 
sekuens_latih = tokenizer.texts_to_sequences(berita_latih)
sekuens_test = tokenizer.texts_to_sequences(berita_test)
 
padded_latih = pad_sequences(sekuens_latih,
                             maxlen=300,
                             padding='post',
                             truncating='post') 
padded_test = pad_sequences(sekuens_test,
                            maxlen=300,
                            padding='post',
                            truncating='post')

import tensorflow as tf
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=5000, output_dim=16),
    tf.keras.layers.LSTM(64),
    tf.keras.layers.Dropout(0.025),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.025),
    tf.keras.layers.Dense(5, activation='softmax')
])
model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy')>0.9):
      print("\nAkurasi telah mencapai >90%!")
      self.model.stop_training = True
callbacks = myCallback()

num_epochs = 200
history = model.fit(padded_latih, label_latih, epochs=num_epochs, 
                    validation_data=(padded_test, label_test), verbose=1, callbacks=[callbacks])

#   Membuat grafik akurasi model
import matplotlib.pyplot as plt
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Grafik Akurasi Model')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

#   Membuat grafik loss model
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Grafik Loss Model')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()